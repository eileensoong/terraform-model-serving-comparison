# module.gcp.module.k8s.module.model-serving.module.us-west4.google_container_node_pool.pool["g2-nonpreempt"]:
resource "google_container_node_pool" "pool" {
    cluster                     = "model-serving"
    id                          = "projects/voiceai-staging/locations/us-west4/clusters/model-serving/nodePools/g2-nonpreempt"
    initial_node_count          = 1
    instance_group_urls         = [
        "https://www.googleapis.com/compute/v1/projects/voiceai-staging/zones/us-west4-a/instanceGroupManagers/gke-model-serving-g2-nonpreempt-92016ba2-grp",
        "https://www.googleapis.com/compute/v1/projects/voiceai-staging/zones/us-west4-c/instanceGroupManagers/gke-model-serving-g2-nonpreempt-9078fb6c-grp",
    ]
    location                    = "us-west4"
    managed_instance_group_urls = [
        "https://www.googleapis.com/compute/v1/projects/voiceai-staging/zones/us-west4-a/instanceGroups/gke-model-serving-g2-nonpreempt-92016ba2-grp",
        "https://www.googleapis.com/compute/v1/projects/voiceai-staging/zones/us-west4-c/instanceGroups/gke-model-serving-g2-nonpreempt-9078fb6c-grp",
    ]
    max_pods_per_node           = 110
    name                        = "g2-nonpreempt"
    name_prefix                 = null
    node_count                  = 0
    node_locations              = [
        "us-west4-a",
        "us-west4-c",
    ]
    project                     = "voiceai-staging"
    version                     = "1.28.15-gke.1020000"

    autoscaling {
        location_policy      = "BALANCED"
        max_node_count       = 1
        min_node_count       = 0
        total_max_node_count = 0
        total_min_node_count = 0
    }

    management {
        auto_repair  = true
        auto_upgrade = false
    }

    network_config {
        create_pod_range     = false
        enable_private_nodes = false
        pod_ipv4_cidr_block  = "10.96.0.0/14"
        pod_range            = "gke-model-serving-pods-76fa98db"
    }

    node_config {
        boot_disk_kms_key           = null
        disk_size_gb                = 100
        disk_type                   = "pd-balanced"
        effective_taints            = [
            {
                effect = "NO_SCHEDULE"
                key    = "nvidia.com/gpu"
                value  = "present"
            },
        ]
        enable_confidential_storage = false
        image_type                  = "COS_CONTAINERD"
        labels                      = {
            "cluster"   = "model-serving"
            "component" = "ai"
            "feature"   = "external"
            "team"      = "ai_eng"
        }
        local_ssd_count             = 0
        logging_variant             = "DEFAULT"
        machine_type                = "g2-standard-24"
        metadata                    = {
            "disable-legacy-endpoints" = "true"
        }
        min_cpu_platform            = null
        node_group                  = null
        oauth_scopes                = [
            "https://www.googleapis.com/auth/cloud-platform",
        ]
        preemptible                 = false
        resource_labels             = {}
        resource_manager_tags       = {}
        service_account             = "cluster-model-serving@voiceai-staging.iam.gserviceaccount.com"
        spot                        = false
        storage_pools               = []
        tags                        = []

        guest_accelerator {
            count              = 2
            gpu_partition_size = null
            type               = "nvidia-l4"
        }

        kubelet_config {
            cpu_cfs_quota                          = false
            cpu_cfs_quota_period                   = null
            cpu_manager_policy                     = null
            insecure_kubelet_readonly_port_enabled = "TRUE"
            pod_pids_limit                         = 0
        }

        shielded_instance_config {
            enable_integrity_monitoring = true
            enable_secure_boot          = false
        }

        taint {
            effect = "NO_SCHEDULE"
            key    = "nvidia.com/gpu"
            value  = "present"
        }

        workload_metadata_config {
            mode = "GKE_METADATA"
        }
    }

    upgrade_settings {
        max_surge       = 1
        max_unavailable = 0
        strategy        = "SURGE"
    }
}
